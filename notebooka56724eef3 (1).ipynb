{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-23T01:50:17.142498Z","iopub.execute_input":"2021-11-23T01:50:17.143808Z","iopub.status.idle":"2021-11-23T01:50:17.159981Z","shell.execute_reply.started":"2021-11-23T01:50:17.143747Z","shell.execute_reply":"2021-11-23T01:50:17.159047Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Plots\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\n%matplotlib inline\n\n#NLP TOOLS\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nstopwords=stopwords.words('english')\n\n\n#Strings and regular expressions\nimport string,re\n\n#Text Vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Classifiers\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Model Building\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#Evaluation\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:50:17.161679Z","iopub.execute_input":"2021-11-23T01:50:17.163513Z","iopub.status.idle":"2021-11-23T01:51:17.289415Z","shell.execute_reply.started":"2021-11-23T01:50:17.163461Z","shell.execute_reply":"2021-11-23T01:51:17.288099Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv')\n\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:51:17.291460Z","iopub.execute_input":"2021-11-23T01:51:17.291805Z","iopub.status.idle":"2021-11-23T01:51:18.573881Z","shell.execute_reply.started":"2021-11-23T01:51:17.291764Z","shell.execute_reply":"2021-11-23T01:51:18.570103Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#fix messy column names\ndf.columns=df.columns.str.replace('/','_')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:51:18.575682Z","iopub.execute_input":"2021-11-23T01:51:18.575948Z","iopub.status.idle":"2021-11-23T01:51:18.586057Z","shell.execute_reply.started":"2021-11-23T01:51:18.575915Z","shell.execute_reply":"2021-11-23T01:51:18.581604Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#remove the non-english titles\ndf['Origin_Ethnicity'].value_counts()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:51:18.589718Z","iopub.execute_input":"2021-11-23T01:51:18.590170Z","iopub.status.idle":"2021-11-23T01:51:18.628971Z","shell.execute_reply.started":"2021-11-23T01:51:18.590099Z","shell.execute_reply":"2021-11-23T01:51:18.628175Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\nmovie_origin=['American', 'British', 'Australian', 'Canadian']\n#df=df['Origin_Ethnicity'].isin(movie_origin)\n\ndf.head()\ndf=df[df.Origin_Ethnicity.isin(movie_origin)]\ndf.head()\ndf.Origin_Ethnicity.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:51:18.630456Z","iopub.execute_input":"2021-11-23T01:51:18.630895Z","iopub.status.idle":"2021-11-23T01:51:18.656876Z","shell.execute_reply.started":"2021-11-23T01:51:18.630855Z","shell.execute_reply":"2021-11-23T01:51:18.656039Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Maybe Changing the order of the genres will streamline the process\ndef alphabetize_genre(a_string):\n    myGenre=a_string.replace('-', ',').replace('/', ',')\n    words=word_tokenize(myGenre)\n    words.sort()\n    \n    for word in words:\n        if (word==','):\n            words.remove(word)\n    myGenre=' '.join(words)\n    return myGenre\n\ndf['Sorted_genre']=df['Genre'].apply(alphabetize_genre)\nprint('Original Genre:', df['Genre'][2000])\nprint('Sorted Genre: ', df['Sorted_genre'][2000])","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:51:18.658440Z","iopub.execute_input":"2021-11-23T01:51:18.658946Z","iopub.status.idle":"2021-11-23T01:51:21.281959Z","shell.execute_reply.started":"2021-11-23T01:51:18.658898Z","shell.execute_reply":"2021-11-23T01:51:21.280909Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Changing the value of genre so it only has 1 values\ndef select_first(a_string):\n    words=word_tokenize(a_string)\n    \n    for word in words:\n        if (word==','):\n            words.remove(word)\n    return words[0]\n\ndf['First_Genre']= df['Sorted_genre'].apply(select_first)\nprint(\"Original Genre: \", df['Sorted_genre'][2000])\nprint(\"First Genre Only: \", df['First_Genre'][2000])","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:51:21.283905Z","iopub.execute_input":"2021-11-23T01:51:21.284498Z","iopub.status.idle":"2021-11-23T01:51:23.589763Z","shell.execute_reply.started":"2021-11-23T01:51:21.284446Z","shell.execute_reply":"2021-11-23T01:51:23.587904Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#df.Sorted_genre.value_counts()\ncomedy_drama= ['comedy', 'drama']\nmyCondition= df['First_Genre'].isin(comedy_drama)\ndf= df[myCondition]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:51:23.591463Z","iopub.execute_input":"2021-11-23T01:51:23.592342Z","iopub.status.idle":"2021-11-23T01:51:23.643406Z","shell.execute_reply.started":"2021-11-23T01:51:23.592285Z","shell.execute_reply":"2021-11-23T01:51:23.642351Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#Make a pipeline to change the genres\n#make lowercase\ndef make_lowercase(a_string):\n    return a_string.lower()\n\nlower_test_string='This IS MY Test String'\nlower_test_string=make_lowercase(lower_test_string)\nprint('Make Lowercase: ' + lower_test_string)\n\n#remove punctuation\ndef remove_punct(a_string):\n    a_string = re.sub(r'[^\\w\\s]','',a_string)\n    return a_string\n\npunct_test_string='Hello!! This is exciting?? No. It, really isnt.'\npunct_test_string=remove_punct(punct_test_string)\nprint('Punctuation Removed: ' +punct_test_string)\n\n#remove stopwords\ndef remove_stopwords(a_string):\n    words=word_tokenize(a_string)\n    \n    valid_words=[]\n    \n    for word in words:\n        if word not in stopwords:\n            valid_words.append(word)\n            \n    a_string=' '.join(valid_words)\n    return a_string\n\nstopwords_test_string='Hey so this is, well its my stopwords test its really neat i guess to me'\nstopwords_test_string=remove_stopwords(stopwords_test_string)\nprint('Remove Stopwords: ' + stopwords_test_string)\n    \n#Break the words\ndef stem_the_words(a_string):\n    \n    porter=PorterStemmer()\n    \n    words=word_tokenize(a_string)\n    \n    valid_words=[]\n    \n    for word in words:\n        stemmed_word=porter.stem(word)\n        valid_words.append(stemmed_word)\n        \n    a_string=' '.join(valid_words)\n    return a_string\n\nstemwords_test_string='You walked and I walk along the walkway. Walking is fun since we walk together'\nstemwords_test_string=stem_the_words(stemwords_test_string)\nprint('Stemmed: '+ stemwords_test_string)\n\n#Why is it dropping \"e\"?","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:51:23.645533Z","iopub.execute_input":"2021-11-23T01:51:23.645933Z","iopub.status.idle":"2021-11-23T01:51:23.666196Z","shell.execute_reply.started":"2021-11-23T01:51:23.645886Z","shell.execute_reply":"2021-11-23T01:51:23.664713Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#this is the pipeline so we dont have to type everything a million times\ndef clean_string_pipeline(a_string):\n    a_string=make_lowercase(a_string)\n    a_string=remove_punct(a_string)\n    a_string=remove_stopwords(a_string)\n    \n    \n    \n    return a_string\n\n#sanity check\npipeline_test_string=\"Hello there! Its a lovely day for a walk, wouldn't you agree Mrs. Smith? I love to garden in my garden. Gardening is so rewarding and you'll agree once you have gardened as well.\"\npipeline_test_string=clean_string_pipeline(pipeline_test_string)\nprint('Clean Sentance: '+ pipeline_test_string)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:51:23.669250Z","iopub.execute_input":"2021-11-23T01:51:23.669687Z","iopub.status.idle":"2021-11-23T01:51:23.723834Z","shell.execute_reply.started":"2021-11-23T01:51:23.669602Z","shell.execute_reply":"2021-11-23T01:51:23.687646Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#apply pipeline to datafram\ndf['Clean_Plot']=df['Plot'].apply(clean_string_pipeline)\n\n#sanity check\nprint(\"Original Text: \"+ df['Plot'][20])\nprint(\"Cleaned Test: \"+ df['Clean_Plot'][20])","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:51:23.726446Z","iopub.execute_input":"2021-11-23T01:51:23.727688Z","iopub.status.idle":"2021-11-23T01:52:06.732657Z","shell.execute_reply.started":"2021-11-23T01:51:23.727635Z","shell.execute_reply":"2021-11-23T01:52:06.731696Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#Select and Engineer Features\n#select X and y\n\nX= df['Clean_Plot'].values\ny=df['First_Genre'].values","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:52:06.734701Z","iopub.execute_input":"2021-11-23T01:52:06.735268Z","iopub.status.idle":"2021-11-23T01:52:06.742403Z","shell.execute_reply.started":"2021-11-23T01:52:06.735212Z","shell.execute_reply":"2021-11-23T01:52:06.741402Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\n\n# This makes your vocab matrix\nvectorizer.fit(X)\n\n# This transforms your documents into vectors.\nX=vectorizer.transform(X)\nprint(X.shape, type(X))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:52:06.746529Z","iopub.execute_input":"2021-11-23T01:52:06.747782Z","iopub.status.idle":"2021-11-23T01:52:12.943810Z","shell.execute_reply.started":"2021-11-23T01:52:06.747701Z","shell.execute_reply":"2021-11-23T01:52:12.942844Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=.20, random_state=20)\n\n#Initialize the model\nmodel=MultinomialNB(alpha=.5)\n#I chose to only use the hyperparameter alpha since I felt like it was the only one that was relevant to this data set. \n#Alpha as a hyperparameter smooths out the model, making everything work better. \n#There arent too many other parameters for this method, but the accuracy with alpha still comes out pretty good. \n\n#fit\nmodel.fit(X_train,y_train)\n\n#test\ny_pred=model.predict(X_test)\n\ny_pred_proba= model.predict_proba(X_test)\n\n#check accuracy\naccuracy=model.score(X_test, y_test)\nprint('Model Accuracy: %f' % accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:52:12.945244Z","iopub.execute_input":"2021-11-23T01:52:12.946089Z","iopub.status.idle":"2021-11-23T01:52:13.082078Z","shell.execute_reply.started":"2021-11-23T01:52:12.946042Z","shell.execute_reply":"2021-11-23T01:52:13.081004Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#Check other scores\nprint(classification_report(y_test, y_pred, target_names=model.classes_))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:52:13.084033Z","iopub.execute_input":"2021-11-23T01:52:13.084378Z","iopub.status.idle":"2021-11-23T01:52:13.234327Z","shell.execute_reply.started":"2021-11-23T01:52:13.084334Z","shell.execute_reply":"2021-11-23T01:52:13.233328Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#plot a confusion matrix\nfig, ax= plt.subplots(figsize=(10,10))\ndisp = plot_confusion_matrix(model, X_test, y_test,\n                             display_labels=model.classes_,\n                             cmap=plt.cm.Blues, ax=ax)\nplt.xticks(rotation=90)\ndisp","metadata":{"execution":{"iopub.status.busy":"2021-11-23T01:52:13.235837Z","iopub.execute_input":"2021-11-23T01:52:13.236697Z","iopub.status.idle":"2021-11-23T01:52:13.675065Z","shell.execute_reply.started":"2021-11-23T01:52:13.236642Z","shell.execute_reply":"2021-11-23T01:52:13.674162Z"},"trusted":true},"execution_count":26,"outputs":[]}]}